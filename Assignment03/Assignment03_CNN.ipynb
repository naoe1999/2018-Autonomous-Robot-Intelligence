{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 주피터 노트북용 tqdm (progress bar 표시용)\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "# 일반 python용 tqdm\n",
    "#from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleCNN:\n",
    "    def __init__(self, lr=0.001):\n",
    "        # initialize convolutional filters (He initialization)\n",
    "        convF1 = np.random.randn(3, 3, 1, 16) / np.sqrt(3*3/2)     # filter size: 3x3x1  x16 channels\n",
    "        convF2 = np.random.randn(3, 3, 16, 32) / np.sqrt(3*3*16/2) # filter size: 3x3x16 x32 channels\n",
    "        convF3 = np.random.randn(3, 3, 32, 64) / np.sqrt(3*3*32/2) # filter size: 3x3x32 x64 channels\n",
    "        \n",
    "        # initialize fully connected layer weights (He initialization)\n",
    "        fcW1 = np.random.randn(7*7*64, 512) / np.sqrt(7*7*64/2)    # shape: 3136 x 512\n",
    "        fcW2 = np.random.randn(512, 10) / np.sqrt(512/2)           # shape: 512 x 10\n",
    "        \n",
    "        # 전체 weights. 편의상 하나로 묶어서 관리\n",
    "        self.weights = np.array([convF1, convF2, convF3, fcW1, fcW2])\n",
    "        \n",
    "        # learning rate\n",
    "        self.learning_rate = lr\n",
    "        \n",
    "        # ADAM hyper-parameters\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.e = 1E-8\n",
    "        self.time_step = 0\n",
    "        # ADAM momentum & RMSProp 저장을 위한 변수\n",
    "        self.m = np.array([np.zeros(self.weights[i].shape) for i in range(len(self.weights))])\n",
    "        self.v = np.array([np.zeros(self.weights[i].shape) for i in range(len(self.weights))])\n",
    "    \n",
    "    def adam_optimization_step(self, gradient):\n",
    "        # update momentum\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * gradient\n",
    "        \n",
    "        # update RMSProp\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * gradient**2\n",
    "        \n",
    "        # unbias\n",
    "        self.time_step += 1\n",
    "        m_hat = self.m / (1 - np.power(self.beta1, self.time_step))\n",
    "        v_hat = self.v / (1 - np.power(self.beta2, self.time_step))\n",
    "        \n",
    "        # update weights\n",
    "        self.weights = self.weights - self.learning_rate * m_hat / (v_hat + self.e)**0.5\n",
    "    \n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    # relu'(x)\n",
    "    def relu_dot(self, x):\n",
    "        return (x >= 0) * 1.0\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        # 지수가 너무 커지는 것을 방지하기 위해 (overflow) 최대값으로 분모, 분자를 나눔\n",
    "        e_x = np.exp(x - x.max(axis=1, keepdims=True))\n",
    "        return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # softmax 함수의 cross entropy 손실함수 sigma { -y ln(y_pred) }\n",
    "    def cross_entropy_loss(self, y_gt, y_pred=None):\n",
    "        if y_pred is None:\n",
    "            y_pred = self.Y_pred\n",
    "        return - (y_gt * np.log(y_pred)).sum() / y_gt.shape[0]  # N개 data에 대한 평균\n",
    "    \n",
    "    def accuracy(self, y_gt, y_pred=None):\n",
    "        if y_pred is None:\n",
    "            y_pred = self.Y_pred\n",
    "        # N개 data에 대해 정답과 예측의 일치 개수 비율을 구함\n",
    "        return np.equal(y_gt.argmax(axis=1), y_pred.argmax(axis=1)).sum() / y_gt.shape[0]\n",
    "    \n",
    "    #############################\n",
    "    # convolutional function\n",
    "    def conv3x3(self, x, f):\n",
    "        # zero-padding\n",
    "        xp = np.zeros((x.shape[0], x.shape[1] +2, x.shape[2] +2, x.shape[3]))\n",
    "        xp[:, 1:-1, 1:-1, :] = x\n",
    "        \n",
    "        # convolutional operation  X:(N, W, H, C) * F:(w, h, C, D) ==> Y:(N, W, H, D)\n",
    "        y = np.zeros((x.shape[0], x.shape[1], x.shape[2], f.shape[3]))\n",
    "        for i in range(y.shape[1]):\n",
    "            for j in range(y.shape[2]):\n",
    "                # x_fraction:(N, w, h, C, 1) * f:(w, h, C, D) --> (N, w, h, C, D) \n",
    "                #  --> 2,3,4차원 합계 --> (N, 1, D)\n",
    "                #  --> W, H 만큼 반복하며 broadcasting --> (N, W, H, D)\n",
    "                y[:, i, j, :] = np.sum(xp[:, i:i+3, j:j+3, :, None] * f, axis=(1,2,3))\n",
    "        return y\n",
    "    \n",
    "    #############################\n",
    "    # convolutional backpropagation (conv. layer를 통한 error map 역전파)\n",
    "    def conv3x3_backprop(self, err_map, f):\n",
    "        # error map과 뒤집힌 filter 사이의 convolutional 연산과 동일함\n",
    "        # filter는 w(0), h(1)를 뒤집고, input channel(2)과 output channel(3)을 뒤집어야 함 (transpose(1,0,3,2))\n",
    "        return self.conv3x3(err_map, f.transpose(1,0,3,2))\n",
    "    \n",
    "    #############################\n",
    "    # convolutional filter gradient 계산\n",
    "    def conv3x3_gradient(self, err_map, x):\n",
    "        # zero-padding\n",
    "        xp = np.zeros((x.shape[0], x.shape[1] +2, x.shape[2] +2, x.shape[3]))\n",
    "        xp[:, 1:-1, 1:-1, :] = x\n",
    "        \n",
    "        # convolutional operation channel by channel  X:(N, W, H, C) * Err:(N, W, H, D) ==> F:(w, h, C, D)\n",
    "        grad_f = np.zeros((3, 3, x.shape[3], err_map.shape[3]))\n",
    "        for i in range(grad_f.shape[0]):\n",
    "            for j in range(grad_f.shape[1]):\n",
    "                # x:(N, W, H, C, 1) * err:(N, W, H, 1, D) --> (N, W, H, C, D)\n",
    "                #  --> 1,2,3차원 합계 --> (1, C, D)\n",
    "                #  --> w, h 만큼 반복 --> (w, h, C, D)\n",
    "                grad_f[i, j, :, :] = np.sum(xp[:, i:i+x.shape[1], j:j+x.shape[2], :, None] * err_map[:, :, :, None, :], axis=(0,1,2))\n",
    "        return grad_f\n",
    "    \n",
    "    #############################\n",
    "    # max-pooling function\n",
    "    def pool2x2(self, x):\n",
    "        # input 크기가 2의 배수라면 reshape와 max 함수로 구현 가능\n",
    "        return x.reshape(x.shape[0], x.shape[1]//2, 2, x.shape[2]//2, 2, x.shape[3]).max(axis=(2, 4))\n",
    "    \n",
    "    #############################\n",
    "    # max-pooling backpropagation (max-pooling layer를 통한 error map 역전파)\n",
    "    def pool2x2_backprop(self, err_map, x, y):\n",
    "        # max-pooling 출력의 dimension이 입력의 demension과 같아지도록 2x2 반복\n",
    "        # 이를 입력 값과 비교하여 값이 같다면 1, 다르다면 0으로 mask 셋팅\n",
    "        mask = np.equal(x, y.repeat(2, axis=1).repeat(2, axis=2))\n",
    "        \n",
    "        # error map 또한 입력 dimension과 같아지도록 2x2 반복\n",
    "        err_map_upscale = err_map.repeat(2, axis=1).repeat(2, axis=2)\n",
    "        \n",
    "        # error map과 mask를 (elementwise) 곱하여 error map 역전파\n",
    "        return mask * err_map_upscale\n",
    "    \n",
    "    \n",
    "    def decay_learning_rate(self, rate):\n",
    "        self.learning_rate *= rate\n",
    "        return self.learning_rate\n",
    "    \n",
    "    def feedforward(self, x):\n",
    "        # input layer\n",
    "        self.X = np.reshape(x, (-1, 28, 28, 1))             # --> (N, H=28, W=28, C=1)\n",
    "        \n",
    "        # convolutional layer 1 - relu activation\n",
    "        self.C1i = self.conv3x3(self.X, self.weights[0])    # --> (N, 28, 28, 16)\n",
    "        self.C1 = self.relu(self.C1i)\n",
    "        # max pooling layer\n",
    "        self.P1 = self.pool2x2(self.C1)                     # --> (N, 14, 14, 16)\n",
    "        \n",
    "        # convolutional layer 2 - relu activation\n",
    "        self.C2i = self.conv3x3(self.P1, self.weights[1])   # --> (N, 14, 14, 32)\n",
    "        self.C2 = self.relu(self.C2i)\n",
    "        # convolutional layer 3 - relu activation\n",
    "        self.C3i = self.conv3x3(self.C2, self.weights[2])   # --> (N, 14, 14, 64)\n",
    "        self.C3 = self.relu(self.C3i)\n",
    "        # max pooling layer\n",
    "        self.P3 = self.pool2x2(self.C3)                     # --> (N, 7, 7, 64)\n",
    "        \n",
    "        # flatten\n",
    "        self.FL = self.P3.reshape(-1, 7*7*64)               # --> (N, 7*7*64)\n",
    "        \n",
    "        # fully connected layer - relu activation\n",
    "        self.FCi = np.dot(self.FL, self.weights[3])         # --> (N, 512)\n",
    "        self.FC = self.relu(self.FCi)\n",
    "        \n",
    "        # output layer - softmax activation\n",
    "        self.Y_pred = self.softmax(np.dot(self.FC, self.weights[4]))  # --> (N, 10)\n",
    "        return self.Y_pred\n",
    "    \n",
    "    \n",
    "    def train_step(self, x, y_gt):\n",
    "        # feedforward\n",
    "        y_pred = self.feedforward(x)\n",
    "        \n",
    "        # loss & accuracy\n",
    "        y = np.reshape(y_gt, (-1, 10))\n",
    "        ff_loss = self.cross_entropy_loss(y, y_pred)\n",
    "        ff_acc = self.accuracy(y, y_pred)\n",
    "        \n",
    "        # backpropagation\n",
    "        #  <-- output layer <--\n",
    "        errmap = (y_pred - y)                   # dL/dy * softmax_dot         # (N, 10)\n",
    "        grad_W2 = np.dot(self.FC.T, errmap)                                   # (512, 10)\n",
    "        errmap = np.dot(errmap, self.weights[4].T)                            # (N, 512) <--\n",
    "        \n",
    "        #  <-- fully connected layer <--\n",
    "        errmap = errmap * self.relu_dot(self.FCi)\n",
    "        grad_W1 = np.dot(self.FL.T, errmap)                                   # (3136, 512)\n",
    "        errmap = np.dot(errmap, self.weights[3].T).reshape(-1, 7, 7, 64)      # (N, 7, 7, 64) <--\n",
    "        \n",
    "        #  <-- max pooling layer <--\n",
    "        errmap = self.pool2x2_backprop(errmap, self.C3, self.P3)              # (N, 14, 14, 64) <--\n",
    "        \n",
    "        #  <-- convolutional layer <--\n",
    "        errmap = errmap * self.relu_dot(self.C3i)\n",
    "        grad_F3 = self.conv3x3_gradient(errmap, self.C2)                      # (3, 3, 32, 64)\n",
    "        errmap = self.conv3x3_backprop(errmap, self.weights[2])               # (N, 14, 14, 32) <--\n",
    "        \n",
    "        #  <-- convolutional layer <--\n",
    "        errmap = errmap * self.relu_dot(self.C2i)\n",
    "        grad_F2 = self.conv3x3_gradient(errmap, self.P1)                      # (3, 3, 16, 32)\n",
    "        errmap = self.conv3x3_backprop(errmap, self.weights[1])               # (N, 14, 14, 16) <--\n",
    "        \n",
    "        #  <-- max pooling layer <--\n",
    "        errmap = self.pool2x2_backprop(errmap, self.C1, self.P1)              # (N, 28, 28, 16) <--\n",
    "        \n",
    "        #  <-- convolutional layer <--\n",
    "        errmap = errmap * self.relu_dot(self.C1i)\n",
    "        grad_F1 = self.conv3x3_gradient(errmap, self.X)                       # (3, 3, 1, 16)\n",
    "        \n",
    "        # 각 gradient는 총 N(=batch_size) 개의 data에 대한 합 형태로 계산됐으므로 N으로 나누어야 함\n",
    "        num_data = y.shape[0]\n",
    "        gradients = np.array([grad_F1, grad_F2, grad_F3, grad_W1, grad_W2]) / num_data\n",
    "        \n",
    "        # optimization (GDM)\n",
    "        #self.weights = self.weights - self.learning_rate * gradients;\n",
    "        \n",
    "        # optimization (ADAM)\n",
    "        self.adam_optimization_step(gradients)\n",
    "        \n",
    "        # feedforward 직후 (학습 step 수행 전) loss와 accuracy 값을 return\n",
    "        return ff_loss, ff_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "epoch = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "decay_rate = 0.8\n",
    "num_training_data = 50000\n",
    "\n",
    "net = SimpleCNN(lr=learning_rate)\n",
    "\n",
    "# training data loading\n",
    "training_dataset_file = open(\"mnist_train.csv\", 'r')\n",
    "training_dataset_list = training_dataset_file.readlines()\n",
    "training_dataset_file.close()\n",
    "\n",
    "# training data의 input과 정답을 각각 list에 저장\n",
    "input_list = list()\n",
    "target_list = list()\n",
    "for i in training_dataset_list:\n",
    "    all_values = i.split(',')\n",
    "    inputs = (np.asfarray(all_values[1:])/255.0*0.99)+0.01\n",
    "    input_list.append(inputs)\n",
    "    \n",
    "    targets = np.zeros(10) + 0.001\n",
    "    targets[int(all_values[0])] = 0.991   # sum to 1\n",
    "    target_list.append(targets)\n",
    "    \n",
    "# training, validation으로 나눔\n",
    "training_input_list = input_list[:num_training_data]\n",
    "training_target_list = target_list[:num_training_data]\n",
    "validation_input_list = input_list[num_training_data:]\n",
    "validation_target_list = target_list[num_training_data:]\n",
    "\n",
    "del(input_list)\n",
    "del(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65acd77a7f6490891bffea9b44c5d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9056271c004c1198902ce14252e762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation accuracy=0.988600, loss=0.119708\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8701334c900448ea8120d3ee3ef91aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f838479f64475c963de46b40f5681c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation accuracy=0.990400, loss=0.112138\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b64342f4d04765903f8f87929701e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a101ee4938d448758868052412a1fcd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation accuracy=0.992700, loss=0.101578\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8abdfdf7a444ee957bf664639c4362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edfc55fc62a248f1a454384497873049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation accuracy=0.994000, loss=0.098220\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcaafe1b1f1a4093bc89eee731476928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e19d4e667084aaa8ffc0c7011a98e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation accuracy=0.994500, loss=0.094369\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea85702cbdbe40baac223cb50a89242d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d662eb2967e44604aede269aa41908ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation accuracy=0.995300, loss=0.092764\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343265d2ff76465da6b817541447bb14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7179985956914e4683487b408843faa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation accuracy=0.994900, loss=0.091492\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb89182f2a042c8b8fc43a880af5bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351ff45a902d4d1cb3b5720c4120c1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation accuracy=0.995600, loss=0.090566\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8b3649041b4b0eb6cc3ddc71a91f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_loss_list = list()\n",
    "validation_loss_list = list()\n",
    "\n",
    "# 전체 반복 횟수 counting을 위한 변수\n",
    "num_iter = 0\n",
    "\n",
    "# epoch loop\n",
    "for k in range(epoch):\n",
    "    # epoch 마다 learning rate 감소\n",
    "    if k != 0:\n",
    "        learning_rate = net.decay_learning_rate(decay_rate)\n",
    "    \n",
    "    # epoch 마다 mini batch 순서 변경\n",
    "    permute_indices = np.random.permutation(len(training_input_list))\n",
    "    \n",
    "    # training용 mini batch 반복 및 progress bar 설정\n",
    "    tr = tqdm(range(0, len(training_input_list), batch_size));\n",
    "    tr.set_description('Training: %i epoch' % (k+1))\n",
    "    \n",
    "    # training loop\n",
    "    # 모든 mini batch에 대해 학습 수행\n",
    "    for i in tr:\n",
    "        batch_indices = permute_indices[i:i+batch_size]\n",
    "        x = [training_input_list[ii] for ii in batch_indices]\n",
    "        y = [training_target_list[ii] for ii in batch_indices]\n",
    "        \n",
    "        # 학습 1회 실시\n",
    "        loss, acc = net.train_step(x, y)\n",
    "        \n",
    "        # 각 반복의 training accuracy, loss, learning rate를 progress bar 화면에 표시\n",
    "        tr.set_postfix(loss=loss, accuracy=acc, learning_rate=learning_rate)\n",
    "        \n",
    "        # 그래프 출력을 위한 training loss 값 저장\n",
    "        training_loss_list.append((num_iter, loss, acc))\n",
    "        num_iter += 1\n",
    "    \n",
    "    \n",
    "    # validation용 mini batch 반복 및 progress bar 설정\n",
    "    va = tqdm(range(0, len(validation_input_list), batch_size));\n",
    "    va.set_description('Validation: %i epoch' % (k+1))\n",
    "    \n",
    "    # validation loop\n",
    "    # mini batch 단위로 수행하여 합계를 구한 후, validation data 개수로 나누어 평균을 구함\n",
    "    va_loss_sum = 0\n",
    "    va_acc_sum = 0\n",
    "    for i in va:\n",
    "        x = validation_input_list[i:i+batch_size]\n",
    "        y = validation_target_list[i:i+batch_size]\n",
    "        # feedforward\n",
    "        y_pred = net.feedforward(x)\n",
    "        y = np.reshape(y, (-1, 10))\n",
    "        # validation loss, accuracy 총합 누적\n",
    "        va_loss_sum += net.cross_entropy_loss(y, y_pred) * len(x)\n",
    "        va_acc_sum += net.accuracy(y, y_pred) * len(x)\n",
    "    \n",
    "    # validation loss, accuracy 평균 계산\n",
    "    va_loss = va_loss_sum / len(validation_input_list)\n",
    "    va_acc = va_acc_sum / len(validation_input_list)\n",
    "    print('validation accuracy=%f, loss=%f' % (va_acc, va_loss))\n",
    "    \n",
    "    # 그래프 출력을 위한 validation loss 값 저장\n",
    "    validation_loss_list.append((num_iter, va_loss, va_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_history = np.array(training_loss_list)\n",
    "va_history = np.array(validation_loss_list)\n",
    "\n",
    "# 학습과정에 따른 loss 변화 그래프\n",
    "# (파란색: training data, 주황색: validation data)\n",
    "fig_loss = plt.plot(tr_history[:,0], tr_history[:,1], va_history[:,0], va_history[:,1])\n",
    "plt.ylim(0, 1.5)\n",
    "plt.show(fig_loss)\n",
    "\n",
    "# 학습과정에 따른 accuracy 변화 그래프\n",
    "# (파란색: training data, 주황색: validation data)\n",
    "fig_acc = plt.plot(tr_history[:,0], tr_history[:,2], va_history[:,0], va_history[:,2])\n",
    "plt.ylim(0.6, 1)\n",
    "plt.show(fig_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "test_dataset_file = open(\"mnist_test.csv\", 'r')\n",
    "test_dataset_list = test_dataset_file.readlines()\n",
    "test_dataset_file.close()\n",
    "output_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test error rate\n",
    "success = 0\n",
    "failure = 0\n",
    "\n",
    "for i in tqdm(test_dataset_list):\n",
    "    all_values = i.split(',')\n",
    "    target = int(all_values[0])\n",
    "    \n",
    "    #inputs = (np.asfarray(all_values[1:])/255.0*0.99)+0.01\n",
    "    prediction_list = net.feedforward(np.asfarray(all_values[1:]))\n",
    "    prediction = np.argmax(prediction_list)\n",
    "    \n",
    "    if target == prediction:\n",
    "        success = success + 1\n",
    "        #print(\"Prediction is successful. (target, predcition) = \", target, prediction)\n",
    "    else:\n",
    "        failure = failure + 1\n",
    "        #print(\"Prediction fails. (target, predcition) = \", target, prediction)\n",
    "        \n",
    "print(\"Recognition error rate = \", (failure/(success+failure)))\n",
    "print(\"Recognition accuracy = \", (success/(success+failure)) * 100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
