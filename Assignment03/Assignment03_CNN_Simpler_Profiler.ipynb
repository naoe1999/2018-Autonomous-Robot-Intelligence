{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 주피터 노트북용 tqdm (progress bar 표시용)\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "# 일반 python용 tqdm\n",
    "#from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleCNN:\n",
    "    def __init__(self, lr=0.001):\n",
    "        # initialize convolutional filters (He initialization)\n",
    "        convF1 = np.random.randn(3, 3, 1, 16) / np.sqrt(3*3/2)     # filter size: 3x3x1  x16 channels\n",
    "        convF2 = np.random.randn(3, 3, 16, 32) / np.sqrt(3*3*16/2) # filter size: 3x3x16 x32 channels\n",
    "        \n",
    "        # initialize fully connected layer weights (He initialization)\n",
    "        fcW1 = np.random.randn(7*7*32, 512) / np.sqrt(7*7*32/2)    # shape: 1568 x 512\n",
    "        fcW2 = np.random.randn(512, 10) / np.sqrt(512/2)           # shape: 512 x 10\n",
    "        \n",
    "        # 전체 weights. 편의상 하나로 묶어서 관리\n",
    "        self.weights = np.array([convF1, convF2, fcW1, fcW2])\n",
    "        \n",
    "        # learning rate\n",
    "        self.learning_rate = lr\n",
    "        \n",
    "        # ADAM hyper-parameters\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.e = 1E-8\n",
    "        self.time_step = 0\n",
    "        # ADAM momentum & RMSProp 저장을 위한 변수\n",
    "        self.m = np.array([np.zeros(self.weights[i].shape) for i in range(len(self.weights))])\n",
    "        self.v = np.array([np.zeros(self.weights[i].shape) for i in range(len(self.weights))])\n",
    "    \n",
    "    def adam_optimization_step(self, gradient):\n",
    "        # update momentum\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * gradient\n",
    "        \n",
    "        # update RMSProp\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * gradient**2\n",
    "        \n",
    "        # unbias\n",
    "        self.time_step += 1\n",
    "        m_hat = self.m / (1 - np.power(self.beta1, self.time_step))\n",
    "        v_hat = self.v / (1 - np.power(self.beta2, self.time_step))\n",
    "        \n",
    "        # update weights\n",
    "        self.weights = self.weights - self.learning_rate * m_hat / (v_hat + self.e)**0.5\n",
    "    \n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    # relu'(x)\n",
    "    def relu_dot(self, x):\n",
    "        return (x >= 0) * 1.0\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        # 지수가 너무 커지는 것을 방지하기 위해 (overflow) 최대값으로 분모, 분자를 나눔\n",
    "        e_x = np.exp(x - x.max(axis=1, keepdims=True))\n",
    "        return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # softmax 함수의 cross entropy 손실함수 sigma { -y ln(y_pred) }\n",
    "    def cross_entropy_loss(self, y_gt, y_pred=None):\n",
    "        if y_pred is None:\n",
    "            y_pred = self.Y_pred\n",
    "        return - (y_gt * np.log(y_pred)).sum() / y_gt.shape[0]  # N개 data에 대한 평균\n",
    "    \n",
    "    def accuracy(self, y_gt, y_pred=None):\n",
    "        if y_pred is None:\n",
    "            y_pred = self.Y_pred\n",
    "        # N개 data에 대해 정답과 예측의 일치 개수 비율을 구함\n",
    "        return np.equal(y_gt.argmax(axis=1), y_pred.argmax(axis=1)).sum() / y_gt.shape[0]\n",
    "    \n",
    "    #############################\n",
    "    # convolutional function\n",
    "    def conv3x3(self, x, f):\n",
    "        # zero-padding\n",
    "        xp = np.zeros((x.shape[0], x.shape[1] +2, x.shape[2] +2, x.shape[3]))\n",
    "        xp[:, 1:-1, 1:-1, :] = x\n",
    "        \n",
    "        # convolutional operation  X:(N, W, H, C) * F:(w, h, C, D) ==> Y:(N, W, H, D)\n",
    "        y = np.zeros((x.shape[0], x.shape[1], x.shape[2], f.shape[3]))\n",
    "        for i in range(y.shape[1]):\n",
    "            for j in range(y.shape[2]):\n",
    "                # x_fraction:(N, w, h, C, 1) * f:(w, h, C, D) --> (N, w, h, C, D) \n",
    "                #  --> 2,3,4차원 합계 --> (N, 1, D)\n",
    "                #  --> W, H 만큼 반복하며 broadcasting --> (N, W, H, D)\n",
    "                y[:, i, j, :] = np.sum(xp[:, i:i+3, j:j+3, :, None] * f, axis=(1,2,3))\n",
    "        return y\n",
    "    \n",
    "    #############################\n",
    "    # convolutional backpropagation (conv. layer를 통한 error map 역전파)\n",
    "    def conv3x3_backprop(self, err_map, f):\n",
    "        # error map과 뒤집힌 filter 사이의 convolutional 연산과 동일함\n",
    "        # filter는 w(0), h(1)를 뒤집고, input channel(2)과 output channel(3)을 뒤집어야 함 (transpose(1,0,3,2))\n",
    "        return self.conv3x3(err_map, f.transpose(1,0,3,2))\n",
    "    \n",
    "    #############################\n",
    "    # convolutional filter gradient 계산\n",
    "    def conv3x3_gradient(self, err_map, x):\n",
    "        # zero-padding\n",
    "        xp = np.zeros((x.shape[0], x.shape[1] +2, x.shape[2] +2, x.shape[3]))\n",
    "        xp[:, 1:-1, 1:-1, :] = x\n",
    "        \n",
    "        # convolutional operation channel by channel  X:(N, W, H, C) * Err:(N, W, H, D) ==> F:(w, h, C, D)\n",
    "        grad_f = np.zeros((3, 3, x.shape[3], err_map.shape[3]))\n",
    "        for i in range(grad_f.shape[0]):\n",
    "            for j in range(grad_f.shape[1]):\n",
    "                # x:(N, W, H, C, 1) * err:(N, W, H, 1, D) --> (N, W, H, C, D)\n",
    "                #  --> 1,2,3차원 합계 --> (1, C, D)\n",
    "                #  --> w, h 만큼 반복 --> (w, h, C, D)\n",
    "                grad_f[i, j, :, :] = np.sum(xp[:, i:i+x.shape[1], j:j+x.shape[2], :, None] * err_map[:, :, :, None, :], axis=(0,1,2))\n",
    "        return grad_f\n",
    "    \n",
    "    #############################\n",
    "    # max-pooling function\n",
    "    def pool2x2(self, x):\n",
    "        # input 크기가 2의 배수라면 reshape와 max 함수로 구현 가능\n",
    "        return x.reshape(x.shape[0], x.shape[1]//2, 2, x.shape[2]//2, 2, x.shape[3]).max(axis=(2, 4))\n",
    "    \n",
    "    #############################\n",
    "    # max-pooling backpropagation (max-pooling layer를 통한 error map 역전파)\n",
    "    def pool2x2_backprop(self, err_map, x, y):\n",
    "        # max-pooling 출력의 dimension이 입력의 demension과 같아지도록 2x2 반복\n",
    "        # 이를 입력 값과 비교하여 값이 같다면 1, 다르다면 0으로 mask 셋팅\n",
    "        mask = np.equal(x, y.repeat(2, axis=1).repeat(2, axis=2))\n",
    "        \n",
    "        # error map 또한 입력 dimension과 같아지도록 2x2 반복\n",
    "        err_map_upscale = err_map.repeat(2, axis=1).repeat(2, axis=2)\n",
    "        \n",
    "        # error map과 mask를 (elementwise) 곱하여 error map 역전파\n",
    "        return mask * err_map_upscale\n",
    "    \n",
    "    \n",
    "    def decay_learning_rate(self, rate):\n",
    "        self.learning_rate *= rate\n",
    "        return self.learning_rate\n",
    "    \n",
    "    def feedforward(self, x):\n",
    "        # input layer\n",
    "        self.X = np.reshape(x, (-1, 28, 28, 1))             # --> (N, H=28, W=28, C=1)\n",
    "        \n",
    "        # convolutional layer 1 - relu activation\n",
    "        self.C1i = self.conv3x3(self.X, self.weights[0])    # --> (N, 28, 28, 16)\n",
    "        self.C1 = self.relu(self.C1i)\n",
    "        # max pooling layer\n",
    "        self.P1 = self.pool2x2(self.C1)                     # --> (N, 14, 14, 16)\n",
    "        \n",
    "        # convolutional layer 2 - relu activation\n",
    "        self.C2i = self.conv3x3(self.P1, self.weights[1])   # --> (N, 14, 14, 32)\n",
    "        self.C2 = self.relu(self.C2i)\n",
    "        # max pooling layer\n",
    "        self.P2 = self.pool2x2(self.C2)                     # --> (N, 7, 7, 32)\n",
    "        \n",
    "        # flatten\n",
    "        self.FL = self.P2.reshape(-1, 7*7*32)               # --> (N, 7*7*32)\n",
    "        \n",
    "        # fully connected layer - relu activation\n",
    "        self.FCi = np.dot(self.FL, self.weights[2])         # --> (N, 512)\n",
    "        self.FC = self.relu(self.FCi)\n",
    "        \n",
    "        # output layer - softmax activation\n",
    "        self.Y_pred = self.softmax(np.dot(self.FC, self.weights[3]))  # --> (N, 10)\n",
    "        return self.Y_pred\n",
    "    \n",
    "    \n",
    "    def train_step(self, x, y_gt):\n",
    "        # feedforward\n",
    "        y_pred = self.feedforward(x)\n",
    "        \n",
    "        # loss & accuracy\n",
    "        y = np.reshape(y_gt, (-1, 10))\n",
    "        ff_loss = self.cross_entropy_loss(y, y_pred)\n",
    "        ff_acc = self.accuracy(y, y_pred)\n",
    "        \n",
    "        # backpropagation\n",
    "        #  <-- output layer <--\n",
    "        errmap = (y_pred - y)                   # dL/dy * softmax_dot         # (N, 10)\n",
    "        grad_W2 = np.dot(self.FC.T, errmap)                                   # (512, 10)\n",
    "        errmap = np.dot(errmap, self.weights[3].T)                            # (N, 512) <--\n",
    "        \n",
    "        #  <-- fully connected layer <--\n",
    "        errmap = errmap * self.relu_dot(self.FCi)\n",
    "        grad_W1 = np.dot(self.FL.T, errmap)                                   # (1568, 512)\n",
    "        errmap = np.dot(errmap, self.weights[2].T).reshape(-1, 7, 7, 32)      # (N, 7, 7, 32) <--\n",
    "        \n",
    "        #  <-- max pooling layer <--\n",
    "        errmap = self.pool2x2_backprop(errmap, self.C2, self.P2)              # (N, 14, 14, 32) <--\n",
    "        \n",
    "        #  <-- convolutional layer <--\n",
    "        errmap = errmap * self.relu_dot(self.C2i)\n",
    "        grad_F2 = self.conv3x3_gradient(errmap, self.P1)                      # (3, 3, 16, 32)\n",
    "        errmap = self.conv3x3_backprop(errmap, self.weights[1])               # (N, 14, 14, 16) <--\n",
    "        \n",
    "        #  <-- max pooling layer <--\n",
    "        errmap = self.pool2x2_backprop(errmap, self.C1, self.P1)              # (N, 28, 28, 16) <--\n",
    "        \n",
    "        #  <-- convolutional layer <--\n",
    "        errmap = errmap * self.relu_dot(self.C1i)\n",
    "        grad_F1 = self.conv3x3_gradient(errmap, self.X)                       # (3, 3, 1, 16)\n",
    "        \n",
    "        # 각 gradient는 총 N(=batch_size) 개의 data에 대한 합 형태로 계산됐으므로 N으로 나누어야 함\n",
    "        num_data = y.shape[0]\n",
    "        gradients = np.array([grad_F1, grad_F2, grad_W1, grad_W2]) / num_data\n",
    "        \n",
    "        # optimization (GDM)\n",
    "        #self.weights = self.weights - self.learning_rate * gradients;\n",
    "        \n",
    "        # optimization (ADAM)\n",
    "        self.adam_optimization_step(gradients)\n",
    "        \n",
    "        # feedforward 직후 (학습 step 수행 전) loss와 accuracy 값을 return\n",
    "        return ff_loss, ff_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "epoch = 1\n",
    "batch_size = 16\n",
    "learning_rate = 0.01\n",
    "\n",
    "net = SimpleCNN(lr=learning_rate)\n",
    "\n",
    "# training data loading\n",
    "training_dataset_file = open(\"mnist_train_200.csv\", 'r')\n",
    "training_dataset_list = training_dataset_file.readlines()\n",
    "training_dataset_file.close()\n",
    "input_list = list()\n",
    "target_list = list()\n",
    "for i in training_dataset_list:\n",
    "    all_values = i.split(',')\n",
    "    inputs = (np.asfarray(all_values[1:])/255.0*0.99)+0.01\n",
    "    input_list.append(inputs)\n",
    "    \n",
    "    targets = np.zeros(10) + 0.001\n",
    "    targets[int(all_values[0])] = 0.991   # sum to 1\n",
    "    target_list.append(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ad7b9465a34d469b44c9c79e632728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " "
     ]
    }
   ],
   "source": [
    "training_loss_list = list()\n",
    "\n",
    "# training loop\n",
    "def run_train_step():    \n",
    "    permute_indices = np.random.permutation(len(input_list))\n",
    "    \n",
    "    tr = tqdm(range(0, len(input_list), batch_size));\n",
    "    for i in tr:\n",
    "        batch_indices = permute_indices[i:i+batch_size]\n",
    "        x = [input_list[ii] for ii in batch_indices]\n",
    "        y = [target_list[ii] for ii in batch_indices]\n",
    "        \n",
    "        loss, acc = net.train_step(x, y)\n",
    "        \n",
    "        tr.set_postfix(loss=loss, accuracy=acc)\n",
    "        training_loss_list.append(loss)\n",
    "        \n",
    "\n",
    "%prun run_train_step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
