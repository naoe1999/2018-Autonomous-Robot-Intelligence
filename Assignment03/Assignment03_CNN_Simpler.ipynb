{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 주피터 노트북용 tqdm (progress bar 표시용)\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "# 일반 python용 tqdm\n",
    "#from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleCNN:\n",
    "    def __init__(self, lr=0.001):\n",
    "        # initialize convolutional filters (He initialization)\n",
    "        convF1 = np.random.randn(3, 3, 1, 16) / np.sqrt(3*3/2)     # filter size: 3x3x1  x16 channels\n",
    "        convF2 = np.random.randn(3, 3, 16, 32) / np.sqrt(3*3*16/2) # filter size: 3x3x16 x32 channels\n",
    "        \n",
    "        # initialize fully connected layer weights (He initialization)\n",
    "        fcW1 = np.random.randn(7*7*32, 512) / np.sqrt(7*7*32/2)    # shape: 1568 x 512\n",
    "        fcW2 = np.random.randn(512, 10) / np.sqrt(512/2)           # shape: 512 x 10\n",
    "        \n",
    "        # 전체 weights. 편의상 하나로 묶어서 관리\n",
    "        self.weights = np.array([convF1, convF2, fcW1, fcW2])\n",
    "        \n",
    "        # learning rate\n",
    "        self.learning_rate = lr\n",
    "        \n",
    "        # ADAM hyper-parameters\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.e = 1E-8\n",
    "        self.time_step = 0\n",
    "        # ADAM momentum & RMSProp 저장을 위한 변수\n",
    "        self.m = np.array([np.zeros(self.weights[i].shape) for i in range(len(self.weights))])\n",
    "        self.v = np.array([np.zeros(self.weights[i].shape) for i in range(len(self.weights))])\n",
    "    \n",
    "    def adam_optimization_step(self, gradient):\n",
    "        # update momentum\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * gradient\n",
    "        \n",
    "        # update RMSProp\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * gradient**2\n",
    "        \n",
    "        # unbias\n",
    "        self.time_step += 1\n",
    "        m_hat = self.m / (1 - np.power(self.beta1, self.time_step))\n",
    "        v_hat = self.v / (1 - np.power(self.beta2, self.time_step))\n",
    "        \n",
    "        # update weights\n",
    "        self.weights = self.weights - self.learning_rate * m_hat / (v_hat + self.e)**0.5\n",
    "    \n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    # relu'(x)\n",
    "    def relu_dot(self, x):\n",
    "        return (x >= 0) * 1.0\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        # 지수가 너무 커지는 것을 방지하기 위해 (overflow) 최대값으로 분모, 분자를 나눔\n",
    "        e_x = np.exp(x - x.max(axis=1, keepdims=True))\n",
    "        return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # softmax 함수의 cross entropy 손실함수 sigma { -y ln(y_pred) }\n",
    "    def cross_entropy_loss(self, y_gt, y_pred=None):\n",
    "        if y_pred is None:\n",
    "            y_pred = self.Y_pred\n",
    "        return - (y_gt * np.log(y_pred)).sum() / y_gt.shape[0]  # N개 data에 대한 평균\n",
    "    \n",
    "    def accuracy(self, y_gt, y_pred=None):\n",
    "        if y_pred is None:\n",
    "            y_pred = self.Y_pred\n",
    "        # N개 data에 대해 정답과 예측의 일치 개수 비율을 구함\n",
    "        return np.equal(y_gt.argmax(axis=1), y_pred.argmax(axis=1)).sum() / y_gt.shape[0]\n",
    "    \n",
    "    #############################\n",
    "    # convolutional function\n",
    "    def conv3x3(self, x, f):\n",
    "        # zero-padding\n",
    "        xp = np.zeros((x.shape[0], x.shape[1] +2, x.shape[2] +2, x.shape[3]))\n",
    "        xp[:, 1:-1, 1:-1, :] = x\n",
    "        \n",
    "        # convolutional operation  X:(N, W, H, C) * F:(w, h, C, D) ==> Y:(N, W, H, D)\n",
    "        y = np.zeros((x.shape[0], x.shape[1], x.shape[2], f.shape[3]))\n",
    "        for i in range(y.shape[1]):\n",
    "            for j in range(y.shape[2]):\n",
    "                # x_fraction:(N, w, h, C, 1) * f:(w, h, C, D) --> (N, w, h, C, D) \n",
    "                #  --> 2,3,4차원 합계 --> (N, 1, D)\n",
    "                #  --> W, H 만큼 반복하며 broadcasting --> (N, W, H, D)\n",
    "                y[:, i, j, :] = np.sum(xp[:, i:i+3, j:j+3, :, None] * f, axis=(1,2,3))\n",
    "        return y\n",
    "    \n",
    "    #############################\n",
    "    # convolutional backpropagation (conv. layer를 통한 error map 역전파)\n",
    "    def conv3x3_backprop(self, err_map, f):\n",
    "        # error map과 뒤집힌 filter 사이의 convolutional 연산과 동일함\n",
    "        # filter는 w(0), h(1)를 뒤집고, input channel(2)과 output channel(3)을 뒤집어야 함 (transpose(1,0,3,2))\n",
    "        return self.conv3x3(err_map, f.transpose(1,0,3,2))\n",
    "    \n",
    "    #############################\n",
    "    # convolutional filter gradient 계산\n",
    "    def conv3x3_gradient(self, err_map, x):\n",
    "        # zero-padding\n",
    "        xp = np.zeros((x.shape[0], x.shape[1] +2, x.shape[2] +2, x.shape[3]))\n",
    "        xp[:, 1:-1, 1:-1, :] = x\n",
    "        \n",
    "        # convolutional operation channel by channel  X:(N, W, H, C) * Err:(N, W, H, D) ==> F:(w, h, C, D)\n",
    "        grad_f = np.zeros((3, 3, x.shape[3], err_map.shape[3]))\n",
    "        for i in range(grad_f.shape[0]):\n",
    "            for j in range(grad_f.shape[1]):\n",
    "                # x:(N, W, H, C, 1) * err:(N, W, H, 1, D) --> (N, W, H, C, D)\n",
    "                #  --> 1,2,3차원 합계 --> (1, C, D)\n",
    "                #  --> w, h 만큼 반복 --> (w, h, C, D)\n",
    "                grad_f[i, j, :, :] = np.sum(xp[:, i:i+x.shape[1], j:j+x.shape[2], :, None] * err_map[:, :, :, None, :], axis=(0,1,2))\n",
    "        return grad_f\n",
    "    \n",
    "    #############################\n",
    "    # max-pooling function\n",
    "    def pool2x2(self, x):\n",
    "        # input 크기가 2의 배수라면 reshape와 max 함수로 구현 가능\n",
    "        return x.reshape(x.shape[0], x.shape[1]//2, 2, x.shape[2]//2, 2, x.shape[3]).max(axis=(2, 4))\n",
    "    \n",
    "    #############################\n",
    "    # max-pooling backpropagation (max-pooling layer를 통한 error map 역전파)\n",
    "    def pool2x2_backprop(self, err_map, x, y):\n",
    "        # max-pooling 출력의 dimension이 입력의 demension과 같아지도록 2x2 반복\n",
    "        # 이를 입력 값과 비교하여 값이 같다면 1, 다르다면 0으로 mask 셋팅\n",
    "        mask = np.equal(x, y.repeat(2, axis=1).repeat(2, axis=2))\n",
    "        \n",
    "        # error map 또한 입력 dimension과 같아지도록 2x2 반복\n",
    "        err_map_upscale = err_map.repeat(2, axis=1).repeat(2, axis=2)\n",
    "        \n",
    "        # error map과 mask를 (elementwise) 곱하여 error map 역전파\n",
    "        return mask * err_map_upscale\n",
    "    \n",
    "    \n",
    "    def decay_learning_rate(self, rate):\n",
    "        self.learning_rate *= rate\n",
    "        return self.learning_rate\n",
    "    \n",
    "    def feedforward(self, x):\n",
    "        # input layer\n",
    "        self.X = np.reshape(x, (-1, 28, 28, 1))             # --> (N, H=28, W=28, C=1)\n",
    "        \n",
    "        # convolutional layer 1 - relu activation\n",
    "        self.C1i = self.conv3x3(self.X, self.weights[0])    # --> (N, 28, 28, 16)\n",
    "        self.C1 = self.relu(self.C1i)\n",
    "        # max pooling layer\n",
    "        self.P1 = self.pool2x2(self.C1)                     # --> (N, 14, 14, 16)\n",
    "        \n",
    "        # convolutional layer 2 - relu activation\n",
    "        self.C2i = self.conv3x3(self.P1, self.weights[1])   # --> (N, 14, 14, 32)\n",
    "        self.C2 = self.relu(self.C2i)\n",
    "        # max pooling layer\n",
    "        self.P2 = self.pool2x2(self.C2)                     # --> (N, 7, 7, 32)\n",
    "        \n",
    "        # flatten\n",
    "        self.FL = self.P2.reshape(-1, 7*7*32)               # --> (N, 7*7*32)\n",
    "        \n",
    "        # fully connected layer - relu activation\n",
    "        self.FCi = np.dot(self.FL, self.weights[2])         # --> (N, 512)\n",
    "        self.FC = self.relu(self.FCi)\n",
    "        \n",
    "        # output layer - softmax activation\n",
    "        self.Y_pred = self.softmax(np.dot(self.FC, self.weights[3]))  # --> (N, 10)\n",
    "        return self.Y_pred\n",
    "    \n",
    "    \n",
    "    def train_step(self, x, y_gt):\n",
    "        # feedforward\n",
    "        y_pred = self.feedforward(x)\n",
    "        \n",
    "        # loss & accuracy\n",
    "        y = np.reshape(y_gt, (-1, 10))\n",
    "        ff_loss = self.cross_entropy_loss(y, y_pred)\n",
    "        ff_acc = self.accuracy(y, y_pred)\n",
    "        \n",
    "        # backpropagation\n",
    "        #  <-- output layer <--\n",
    "        errmap = (y_pred - y)                   # dL/dy * softmax_dot         # (N, 10)\n",
    "        grad_W2 = np.dot(self.FC.T, errmap)                                   # (512, 10)\n",
    "        errmap = np.dot(errmap, self.weights[3].T)                            # (N, 512) <--\n",
    "        \n",
    "        #  <-- fully connected layer <--\n",
    "        errmap = errmap * self.relu_dot(self.FCi)\n",
    "        grad_W1 = np.dot(self.FL.T, errmap)                                   # (1568, 512)\n",
    "        errmap = np.dot(errmap, self.weights[2].T).reshape(-1, 7, 7, 32)      # (N, 7, 7, 32) <--\n",
    "        \n",
    "        #  <-- max pooling layer <--\n",
    "        errmap = self.pool2x2_backprop(errmap, self.C2, self.P2)              # (N, 14, 14, 32) <--\n",
    "        \n",
    "        #  <-- convolutional layer <--\n",
    "        errmap = errmap * self.relu_dot(self.C2i)\n",
    "        grad_F2 = self.conv3x3_gradient(errmap, self.P1)                      # (3, 3, 16, 32)\n",
    "        errmap = self.conv3x3_backprop(errmap, self.weights[1])               # (N, 14, 14, 16) <--\n",
    "        \n",
    "        #  <-- max pooling layer <--\n",
    "        errmap = self.pool2x2_backprop(errmap, self.C1, self.P1)              # (N, 28, 28, 16) <--\n",
    "        \n",
    "        #  <-- convolutional layer <--\n",
    "        errmap = errmap * self.relu_dot(self.C1i)\n",
    "        grad_F1 = self.conv3x3_gradient(errmap, self.X)                       # (3, 3, 1, 16)\n",
    "        \n",
    "        # 각 gradient는 총 N(=batch_size) 개의 data에 대한 합 형태로 계산됐으므로 N으로 나누어야 함\n",
    "        num_data = y.shape[0]\n",
    "        gradients = np.array([grad_F1, grad_F2, grad_W1, grad_W2]) / num_data\n",
    "        \n",
    "        # optimization (GDM)\n",
    "        #self.weights = self.weights - self.learning_rate * gradients;\n",
    "        \n",
    "        # optimization (ADAM)\n",
    "        self.adam_optimization_step(gradients)\n",
    "        \n",
    "        # feedforward 직후 (학습 step 수행 전) loss와 accuracy 값을 return\n",
    "        return ff_loss, ff_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "epoch = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "decay_rate = 0.8\n",
    "num_training_data = 50000\n",
    "\n",
    "net = SimpleCNN(lr=learning_rate)\n",
    "\n",
    "# training data loading\n",
    "training_dataset_file = open(\"mnist_train.csv\", 'r')\n",
    "training_dataset_list = training_dataset_file.readlines()\n",
    "training_dataset_file.close()\n",
    "\n",
    "# training data의 input과 정답을 각각 list에 저장\n",
    "input_list = list()\n",
    "target_list = list()\n",
    "for i in training_dataset_list:\n",
    "    all_values = i.split(',')\n",
    "    inputs = (np.asfarray(all_values[1:])/255.0*0.99)+0.01\n",
    "    input_list.append(inputs)\n",
    "    \n",
    "    targets = np.zeros(10) + 0.001\n",
    "    targets[int(all_values[0])] = 0.991   # sum to 1\n",
    "    target_list.append(targets)\n",
    "    \n",
    "# training, validation으로 나눔\n",
    "training_input_list = input_list[:num_training_data]\n",
    "training_target_list = target_list[:num_training_data]\n",
    "validation_input_list = input_list[num_training_data:]\n",
    "validation_target_list = target_list[num_training_data:]\n",
    "\n",
    "del(input_list)\n",
    "del(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a088b2c57cdc40dc84b98c360297d1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4fbc93d76d454ba0fd17ec1344820b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation accuracy=0.985600, loss=0.131851\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0ad25edcd14250b7acbc3920a0b005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7cab73f8ba14b0087117d8b0c65c84b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation accuracy=0.990200, loss=0.116541\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e1c1e60384401dbc3b19302582ef34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50c56afb8bc4aa19c40ce4b1c6f447e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation accuracy=0.988700, loss=0.116153\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7468044e66ab4f83a84832efd57deffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9629b5df6824f4a9ec61adb041e1038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation accuracy=0.991800, loss=0.105831\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0650e10337cc413ba4f863a50c377895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d59544ebbf468a9deb22aa69c5a17f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation accuracy=0.992800, loss=0.102014\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a39e5ffc63e4d4980a925ea39682c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1742ca30766040f8b7c5a1b352c5f497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation accuracy=0.993000, loss=0.099741\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf5b6498178c4b1588b61e3b35660e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5394bc4b87ba4295b1f3c00bf2f6f76a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation accuracy=0.993600, loss=0.097547\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e064186feec4dc5b16fd34e7cb8a743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fee82e58d0747e483d7c725e86c452c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation accuracy=0.993800, loss=0.097079\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df45a6a87e6b4ed3a4c6968424a21e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56761cc2ded4f47a60fdc5d0434b2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation accuracy=0.994000, loss=0.095878\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794a82ecab864b29930950584ee0c447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a6542691364be8ab36a78298646425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation accuracy=0.994000, loss=0.095508\n"
     ]
    }
   ],
   "source": [
    "training_loss_list = list()\n",
    "validation_loss_list = list()\n",
    "\n",
    "# 전체 반복 횟수 counting을 위한 변수\n",
    "num_iter = 0\n",
    "\n",
    "# epoch loop\n",
    "for k in range(epoch):\n",
    "    # epoch 마다 learning rate 감소\n",
    "    if k != 0:\n",
    "        learning_rate = net.decay_learning_rate(decay_rate)\n",
    "    \n",
    "    # epoch 마다 mini batch 순서 변경\n",
    "    permute_indices = np.random.permutation(len(training_input_list))\n",
    "    \n",
    "    # training용 mini batch 반복 및 progress bar 설정\n",
    "    tr = tqdm(range(0, len(training_input_list), batch_size));\n",
    "    tr.set_description('Training: %i epoch' % (k+1))\n",
    "    \n",
    "    # training loop\n",
    "    # 모든 mini batch에 대해 학습 수행\n",
    "    for i in tr:\n",
    "        batch_indices = permute_indices[i:i+batch_size]\n",
    "        x = [training_input_list[ii] for ii in batch_indices]\n",
    "        y = [training_target_list[ii] for ii in batch_indices]\n",
    "        \n",
    "        # 학습 1회 실시\n",
    "        loss, acc = net.train_step(x, y)\n",
    "        \n",
    "        # 각 반복의 training accuracy, loss, learning rate를 progress bar 화면에 표시\n",
    "        tr.set_postfix(loss=loss, accuracy=acc, learning_rate=learning_rate)\n",
    "        \n",
    "        # 그래프 출력을 위한 training loss 값 저장\n",
    "        training_loss_list.append((num_iter, loss, acc))\n",
    "        num_iter += 1\n",
    "    \n",
    "    \n",
    "    # validation용 mini batch 반복 및 progress bar 설정\n",
    "    va = tqdm(range(0, len(validation_input_list), batch_size));\n",
    "    va.set_description('Validation: %i epoch' % (k+1))\n",
    "    \n",
    "    # validation loop\n",
    "    # mini batch 단위로 수행하여 합계를 구한 후, validation data 개수로 나누어 평균을 구함\n",
    "    va_loss_sum = 0\n",
    "    va_acc_sum = 0\n",
    "    for i in va:\n",
    "        x = validation_input_list[i:i+batch_size]\n",
    "        y = validation_target_list[i:i+batch_size]\n",
    "        # feedforward\n",
    "        y_pred = net.feedforward(x)\n",
    "        y = np.reshape(y, (-1, 10))\n",
    "        # validation loss, accuracy 총합 누적\n",
    "        va_loss_sum += net.cross_entropy_loss(y, y_pred) * len(x)\n",
    "        va_acc_sum += net.accuracy(y, y_pred) * len(x)\n",
    "    \n",
    "    # validation loss, accuracy 평균 계산\n",
    "    va_loss = va_loss_sum / len(validation_input_list)\n",
    "    va_acc = va_acc_sum / len(validation_input_list)\n",
    "    print('validation accuracy=%f, loss=%f' % (va_acc, va_loss))\n",
    "    \n",
    "    # 그래프 출력을 위한 validation loss 값 저장\n",
    "    validation_loss_list.append((num_iter, va_loss, va_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD8CAYAAACW/ATfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiJFQMleRKQIKNmLiBSB/wUTrdZnYA6iVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d27884ecc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD8CAYAAAB6paOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmeWcE4GZWc45EZiZ5dz/B2eTIYoyRoVPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d2788ba240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tr_history = np.array(training_loss_list)\n",
    "va_history = np.array(validation_loss_list)\n",
    "\n",
    "# 학습과정에 따른 loss 변화 그래프\n",
    "# (파란색: training data, 주황색: validation data)\n",
    "fig_loss = plt.plot(tr_history[:,0], tr_history[:,1], va_history[:,0], va_history[:,1])\n",
    "plt.ylim(0, 1.5)\n",
    "plt.show(fig_loss)\n",
    "\n",
    "# 학습과정에 따른 accuracy 변화 그래프\n",
    "# (파란색: training data, 주황색: validation data)\n",
    "fig_acc = plt.plot(tr_history[:,0], tr_history[:,2], va_history[:,0], va_history[:,2])\n",
    "plt.ylim(0.6, 1)\n",
    "plt.show(fig_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "test_dataset_file = open(\"mnist_test.csv\", 'r')\n",
    "test_dataset_list = test_dataset_file.readlines()\n",
    "test_dataset_file.close()\n",
    "output_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e2293ae6e3c40a68c7907f2e2102db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reconition error rate =  0.0068\n",
      "Reconition accuracy =  99.32 %\n"
     ]
    }
   ],
   "source": [
    "# test error rate\n",
    "success = 0\n",
    "failure = 0\n",
    "\n",
    "for i in tqdm(test_dataset_list):\n",
    "    all_values = i.split(',')\n",
    "    target = int(all_values[0])\n",
    "    \n",
    "    #inputs = (np.asfarray(all_values[1:])/255.0*0.99)+0.01\n",
    "    prediction_list = net.feedforward(np.asfarray(all_values[1:]))\n",
    "    prediction = np.argmax(prediction_list)\n",
    "    \n",
    "    if target == prediction:\n",
    "        success = success + 1\n",
    "        #print(\"Prediction is successful. (target, predcition) = \", target, prediction)\n",
    "    else:\n",
    "        failure = failure + 1\n",
    "        #print(\"Prediction fails. (target, predcition) = \", target, prediction)\n",
    "        \n",
    "print(\"Recognition error rate = \", (failure/(success+failure)))\n",
    "print(\"Recognition accuracy = \", (success/(success+failure)) * 100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
